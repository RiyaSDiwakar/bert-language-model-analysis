# BERT Language Model Analysis

This project explores the capabilities of the BERT language model for
natural language understanding using a Jupyter Notebook.

## Objectives
- Study contextual understanding in transformer-based models
- Analyze how BERT processes different sentence structures
- Identify strengths and limitations of pre-trained language models

## Tools and Technologies
- Python
- Jupyter Notebook
- Hugging Face Transformers
- PyTorch
- Matplotlib

## Key Observations
- BERT captures context effectively using bidirectional attention
- The same word is interpreted differently based on sentence context
- Sentence length impacts tokenization and internal representations

## Conclusion
This project demonstrates the effectiveness of BERT as a foundational
language model in NLP. The analysis provides insight into how modern
language models understand text and where future improvements such as
fine-tuning can enhance performance.
